{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import argparse\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLama 3-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Goal: Memorizing the study list, where the study list consists of person-location pairs (formatted as \"the person is in the location.\"). Please recall and list all the person-location pairs you have memorized in the study list. \n",
    "\n",
    "Input: The study list and question list. Study list is [  \n",
    "The actor is in the airport,\n",
    "The actor is in the airport,\n",
    "The actor is in the airport,\n",
    "The actor is in the airport,\n",
    "The actor is in the airport,\n",
    "The chef is in the attic,\n",
    "The coach is in the bank,\n",
    "The cowboy is in the barn,\n",
    "The dancer is in the castle,\n",
    "The detective is in the church,\n",
    "The doctor is in the clinic,\n",
    "The engineer is in the factory,\n",
    "The farmer is in the garage,\n",
    "The gardener is in the hallway,\n",
    "The hippie is in the hotel,\n",
    "The inventor is in the kitchen,\n",
    "The judge is in the laboratory,\n",
    "The maid is in the library,\n",
    "The musician is in the museum,\n",
    "The pilot is in the nightclub,\n",
    "The pirate is in the office,\n",
    "The queen is in the park,\n",
    "The scientist is in the prison,\n",
    "The sheriff is in the school,\n",
    "The soldier is in the stadium,\n",
    "The spy is in the studio,\n",
    "The teacher is in the temple,\n",
    "The tourist is in the theater,\n",
    "The accountant is in the beach,\n",
    "The artist is in the cinema,\n",
    "The dentist is in the hospital,\n",
    "The nurse is in the restaurant,\n",
    "The writer is in the station,\n",
    "The barista is in the mall,\n",
    "The firefighter is in the zoo,\n",
    "The lawyer is in the pharmacy\n",
    "]. \n",
    "    \n",
    "\n",
    "Output: The results of the analysis need to be in CSV (comma-separated values). \n",
    "Present the results inside a code box.  \n",
    "The code box should be labeled answers_csv \n",
    "Each line of the output should contain the order and the corresponding answer.\n",
    "\n",
    "Analysis: \n",
    "Here is a template describing how to list the pairs.\n",
    "- 1: actor, airport\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16b6c219b4048b2ac79c7d46aa5585c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the code box with the results in CSV format:\n",
      "\n",
      "```\n",
      "answers_csv\n",
      "1: The actor, airport\n",
      "2: The chef, attic\n",
      "3: The coach, bank\n",
      "4: The cowboy, barn\n",
      "5: The dancer, castle\n",
      "6: The detective, church\n",
      "7: The doctor, clinic\n",
      "8: The engineer, factory\n",
      "9: The farmer, garage\n",
      "10: The gardener, hallway\n",
      "11: The hippie, hotel\n",
      "12: The inventor, kitchen\n",
      "13: The judge, laboratory\n",
      "14: The maid, library\n",
      "15: The musician, museum\n",
      "16: The pilot, nightclub\n",
      "17: The pirate, office\n",
      "18: The queen, park\n",
      "19: The scientist, prison\n",
      "20: The sheriff, school\n",
      "21: The soldier, stadium\n",
      "22: The spy, studio\n",
      "23: The teacher, temple\n",
      "24: The tourist, theater\n",
      "25: The accountant, beach\n",
      "26: The artist, cinema\n",
      "27: The dentist, hospital\n",
      "28: The nurse, restaurant\n",
      "29: The writer, station\n",
      "30: The barista, mall\n",
      "31: The firefighter, zoo\n",
      "32: The lawyer, pharmacy\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=5000,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True\n",
    "    #temperature=0.6,\n",
    "    #top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
